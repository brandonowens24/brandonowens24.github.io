[
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Handshake Timeline Reporting\n\n\n\n\n\n\nOakland University\n\n\nPython\n\n\nOOP\n\n\nPowerBI\n\n\n\nA script that uses data from Handshake reports to create a Timeline of all student engagements from 2019-present. With this timeline, the user has the ability to generate several other reports.\n\n\n\n\n\nFeb 25, 2025\n\n\nBrandon Owens\n\n\n\n\n\n\n\n\n\n\n\n\nNBA Moneyline Predictor\n\n\n\n\n\n\nNBA\n\n\nPython\n\n\nAWS\n\n\nDatabase\n\n\nSports\n\n\nModelling\n\n\nWeb-Scraping\n\n\nSports Betting\n\n\n\nScripts used to WebScrape NBA player, team, and officiating information that is uploaded daily to an AWS:RDS instance and used to predict the moneyline of daily NBA games.\n\n\n\n\n\nJan 2, 2025\n\n\nBrandon Owens\n\n\n\n\n\n\n\n\n\n\n\n\nMarch Madness Linear Modelling\n\n\n\n\n\n\nNCAAM\n\n\nSAS\n\n\nRegression\n\n\nSports\n\n\nModelling\n\n\n\nA small project looking to take input variables from NCAAM teams and use them to predict the number of WINS a team will have during a March Madness Tournament.\n\n\n\n\n\nDec 1, 2024\n\n\nBrandon Owens\n\n\n\n\n\n\n\n\n\n\n\n\nShotQuality 3-Point Kaggle Competition\n\n\n\n\n\n\nNCAAM\n\n\nPython\n\n\nCompetition\n\n\nSports\n\n\nModelling\n\n\n\nA submission to a ShotQuality Kaggle Competition that looks to quantify the scheme and ability components that lead to the lowest 3PT% on individual shots.\n\n\n\n\n\nOct 31, 2024\n\n\nBrandon Owens\n\n\n\n\n\n\n\n\n\n\n\n\nSection 8 Housing Fair Market Analyzer\n\n\n\n\n\n\nPython\n\n\nWeb-Scraping\n\n\nTableau\n\n\nHomelessness\n\n\n\nA program that scrapes a list of properties that qualify for Section 8 Housing and evaluate whether or not they are charging Fair Market Rates in the State of Michigan before mapping them.\n\n\n\n\n\nJul 1, 2024\n\n\nBrandon Owens\n\n\n\n\n\n\n\n\n\n\n\n\nNFL Injuries Bayesian Analysis\n\n\n\n\n\n\nNFL\n\n\nBayesian\n\n\nR\n\n\nSports\n\n\nModelling\n\n\n\nA Bayesian Analysis that analyzes the impact of rule changes and age over time with relation to injury rates.\n\n\n\n\n\nApr 10, 2024\n\n\nBrandon Owens\n\n\n\n\n\n\n\n\n\n\n\n\nMovie Recommendation App\n\n\n\n\n\n\nPython\n\n\nFilm\n\n\nRecommender Systems\n\n\nNLP\n\n\n\nA model hosted on HuggingFace spaces that allows you to enter a movie or show you enjoy and receive 5 recommendations!\n\n\n\n\n\nMar 1, 2024\n\n\nBrandon Owens\n\n\n\n\n\n\n\n\n\n\n\n\nElectoral College Bayesian Inference 2016 Election\n\n\n\n\n\n\nBayesian\n\n\nR\n\n\nPolitics\n\n\nModelling\n\n\nStatistics\n\n\n\nUsing conjugate normalized models to analyze predictions for the 2016 Presidential Election via surveying and the Electoral College.\n\n\n\n\n\nFeb 20, 2024\n\n\nBrandon Owens\n\n\n\n\n\n\n\n\n\n\n\n\nSupreme Court Decision Predictor\n\n\n\n\n\n\nPython\n\n\nNLP\n\n\nLaw\n\n\nModelling\n\n\n\nModel that looks to predict the outcome of major Supreme Court Decisions using identity factors and NLP processing.\n\n\n\n\n\nFeb 7, 2024\n\n\nBrandon Owens, Hailey Naugle\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/projects/ShotQuality Kaggle Competition/shotquality.html",
    "href": "projects/projects/ShotQuality Kaggle Competition/shotquality.html",
    "title": "ShotQuality 3-Point Kaggle Competition",
    "section": "",
    "text": "The following notebook consisted of my submission for ShotQuality’s Kaggle and looked to compete within the following prompt:"
  },
  {
    "objectID": "projects/projects/ShotQuality Kaggle Competition/shotquality.html#description",
    "href": "projects/projects/ShotQuality Kaggle Competition/shotquality.html#description",
    "title": "ShotQuality 3-Point Kaggle Competition",
    "section": "Description",
    "text": "Description\nShotQuality is a cutting-edge basketball data company that leverages AI and computer vision to extract precise location data from video feeds for any shot. Our data is trusted by top NCAA teams and sportsbooks to gain a competitive edge. We are excited to offer you an exclusive glimpse into our NCAAM player tracking data, which is used to analyze player and team performance. ShotQuality is the first company to provide player tracking data in all of NCAAM.\n3PT defense has been notoriously difficult to predict on a game-by-game basis and is often plagued by significant noise from season to season, especially in NCAAM. While outliers like Houston have consistently ranked in the top 15 for defensive 3PT% over the past six seasons, the factors that contribute to effective 3PT defense remain largely elusive. Traditionally, 3PT defense has been attributed to a mix of luck, defensive scheme, and player ability. However, in the era of box score statistics, accurately measuring scheme and ability was impossible, leaving luck as the default explanation.\nWe are challenging you to quantify the scheme and ability components that lead to the lowest 3PT% on individual shots. In this year’s Kaggle competition, we invite data scientists, machine learning enthusiasts, and statisticians to explore the intricate in-game contexts that contribute to effective 3PT defense. We aim to understand the impact of defensive and offensive movements using tracking data, including x/y coordinates of all 10 players in the broadcast view, captured up to 4 seconds before the shot attempt and 2 seconds after (180 frames in total) to predict 3PT shots.\nParticipants will be provided with a full month of tracking data to predict the outcome of individual 3PT shots. We encourage you to think creatively, embrace innovative methodologies, and join us in redefining the analysis of 3PT shots. May the most insightful model win!"
  },
  {
    "objectID": "projects/projects/ShotQuality Kaggle Competition/shotquality.html#the-data",
    "href": "projects/projects/ShotQuality Kaggle Competition/shotquality.html#the-data",
    "title": "ShotQuality 3-Point Kaggle Competition",
    "section": "The Data",
    "text": "The Data\nThis competition uses two different types of datasets. Play-by-play data, often abbreviated “pbp”, contains play-level details. This includes teams names and, for the training set, whether the shot is made. We also use location data, often abbreviated “locs”, as the focus on this challenge. The location data includes the on-court location of each player, along with an indicator to show whether the player is the shooter, an offensive player, or a defensive player.\nNote that all shots in the train or test set are misses that are rebounded by the offensive or defensive team. A shot is from the coordinates where annotation_code == “s” in train_locs.csv and test_locs.csv.\nCoordinates for key locations on the court are (position | x,y)"
  },
  {
    "objectID": "projects/projects/ShotQuality Kaggle Competition/shotquality.html#the-process",
    "href": "projects/projects/ShotQuality Kaggle Competition/shotquality.html#the-process",
    "title": "ShotQuality 3-Point Kaggle Competition",
    "section": "The Process",
    "text": "The Process\nMy submission notebook followed the data science process while trying to predict the outcome of specific possessions: 1. Load In Data 2. EDA Part I: Univariate Analysis and Cleaning 3. Train-Test Split 4. Frame Counts per Play 5. Target Percentages 6. Distance to the Basket 7. Filtering 8. Shot Distances Histogram 9. Shooting Densities Heatmap 10. The Null Model 11. EDA Part II: Multivariate Analysis and Advanced Feature Engineering 12. Binned Distance Shooting Percentages 13. Shooting Percentages vs Court Coordinates 14. Advanced Feature Engineering: Distances, Angles, and Player Relativity 15. Floor Spacing 16. Defensive Density 17. Player Distances and Speed Over Time 18. Missing Values 19. Modeling"
  },
  {
    "objectID": "projects/projects/ShotQuality Kaggle Competition/shotquality.html#outcome",
    "href": "projects/projects/ShotQuality Kaggle Competition/shotquality.html#outcome",
    "title": "ShotQuality 3-Point Kaggle Competition",
    "section": "Outcome",
    "text": "Outcome\nI ended up capturing a fourth place finish with the better of my 2 submissions achieving a 0.5287 ROC-AUC Score, and my best model overall achieving a 0.5381 ROC-AUC score."
  },
  {
    "objectID": "projects/projects/NBA Moneyline Predictor/nba_ml.html",
    "href": "projects/projects/NBA Moneyline Predictor/nba_ml.html",
    "title": "NBA Moneyline Predictor",
    "section": "",
    "text": "DUE TO THE COSTS OF INITIATING AWS CLOUDWATCH JOBS, THIS PROJECT HAS BEEN PRIVATIZED\n\nDescription\nThis project involves the collection of daily NBA data including:\n\nOfficials officiating.\nGames being played that night.\nStatistics from games played the previous night.\nUpdated player stats.\nUpdated team stats.\n\nThis aggregation of data is implemented using an AWS Lambda Function. This data is collected nightly via AWS Cloudwatch where it is stored in an AWS RDS SQL Instance. It is then pulled to use evaluation to predict nightly moneylines for occurring NBA games."
  },
  {
    "objectID": "projects/projects/March Madness Linear Models I/mm_lm1.html",
    "href": "projects/projects/March Madness Linear Models I/mm_lm1.html",
    "title": "March Madness Linear Modelling",
    "section": "",
    "text": "The NCAA Men’s March Madness Basketball Tournament is a notoriously difficult tournament to predict. In fact, the best Brackateers in history have never called every game correctly through the first three rounds. Why is it so difficult? Maybe it’s great coaching, a brotherhood formed by experienced teams, or perhaps there is something in the air during March – nobody is certain. In an attempt to maybe one day craft the perfect bracket, the aim of this assignment is to build a model that successfully can predict the number of tournament wins a team will acquire come March."
  },
  {
    "objectID": "projects/projects/March Madness Linear Models I/mm_lm1.html#introduction",
    "href": "projects/projects/March Madness Linear Models I/mm_lm1.html#introduction",
    "title": "March Madness Linear Modelling",
    "section": "",
    "text": "The NCAA Men’s March Madness Basketball Tournament is a notoriously difficult tournament to predict. In fact, the best Brackateers in history have never called every game correctly through the first three rounds. Why is it so difficult? Maybe it’s great coaching, a brotherhood formed by experienced teams, or perhaps there is something in the air during March – nobody is certain. In an attempt to maybe one day craft the perfect bracket, the aim of this assignment is to build a model that successfully can predict the number of tournament wins a team will acquire come March."
  },
  {
    "objectID": "projects/projects/March Madness Linear Models I/mm_lm1.html#the-data",
    "href": "projects/projects/March Madness Linear Models I/mm_lm1.html#the-data",
    "title": "March Madness Linear Modelling",
    "section": "The Data",
    "text": "The Data\nThe dataset utilized within this project was taken from Nishaan Amin’s March Madness Data dataset on Kaggle. Of the numerous tables provided, only the Heat Check Tournament Index and KenPom Barttorvik csvs were taken. The Heat Check Tournament Index dataset is composed of March Madness data after tournament seeding has been conducted, but before any games have been played. Relevant information includes POWER and PATH – referencing a power rating of each team and a rating of the difficulty of their path to the championship, respectively.\nMeanwhile, the KenPom Barttorvik dataset includes the following relevant information KADJ.O (adjusted offense rating), KADJ.D (adjusted defensive rating), BARTHAG (projected win percentage against average D1 team at a neutral site), WIN. (win percentage), FTR (rate of possessions resulting in free throws), TOV. (rate of possessions resulting in turnovers on offense), TOV.D (rate of possessions resulting in turnovers on defense), OREB. (rate of second missed shots resulting in offensive rebounds, DREB. (rate of missed shots resulting in defensive rebounds), X3PT. (three point field goal percentage), X3PT.D (three point field goal percentage against defense), AST. (rate of possessions resultant of an assist), EXP (rating of a team’s experience), and ELITE.SOS (rating of a team’s strength of schedule).\nThese datasets were merged in R on the identifiers YEAR, TEAM.NO, TEAM, SEED, ROUND. After merging, all null values were omitted from the dataset and only the desired columns were taken. This resulted in a new dataset of 640 observations with 20 total variables (17 explanatory variables, 2 identifier variables, and 1 target variable). The target variable/observation of interest was WINS – which means the number of wins a team had in the tournament. For example, if a team lost their first game and was eliminated, they would have 0 WINS. At the same time, a team with 6 WINS equates to the national champion for that year with wins in the round of 64, 32, Sweet 16, Elite 8, Final 4, and the championship."
  },
  {
    "objectID": "projects/projects/March Madness Linear Models I/mm_lm1.html#the-process",
    "href": "projects/projects/March Madness Linear Models I/mm_lm1.html#the-process",
    "title": "March Madness Linear Modelling",
    "section": "The Process",
    "text": "The Process\nAfter importing the data to SAS, the 640 observations were split with a sample rate of 95%. This left 32 observations to be excluded on the dataset – not for testing, but instead to use as an example of future prediction intervals. With these other 608 observations, a linear model was regressed where different metrics and plots were recorded, such as RSQUARE, ADJRSQ, AIC, SBC, PRESS, and residual plots. Each of the metrics performed poorly (0.3943, 0.3769, 75.0403, 154.423, 687.903, respectively) and the fit diagnostics demonstrated non-constant error variance and non-independence of errors for many of the predictor variables. Additionally, many of the explanatory variables appeared non-significant by their respective Fisher tests. Thus, the first focus was on multicollinearity. Variance Inflation Factors were calculated (and removed if larger than 10) and two of the metrics: KADJ_O, KADJ_D, were combined. The resulting variables were WIN_, FTR, TOV_, TOV_D, OREB_, DREB_, X3PT_, X3PT_D, AST_, EXP, PATH, ELITE_SOS, and KADJ_COMB.\nNext, to further improve this model it was beneficial to address heteroscedasticity and assume constant error variance by regressing the model on Weighted Least Squares. The weight equation was taken calculating the inverse of the residuals due to the fact that the original model had many outliers. After doing so, the diagnostic metrics had significantly improved (RSQUARE, ADJRSQ, AIC, SBC, PRESS equal to 0.73639, 0.73062, -100.374, -38.6314, and 503.535, respectively). Additionally, the diagnostic plots appeared to significantly improve upon the previous non-constancy of error variance. The next step was to determine which variables were significantly interacting with one another and contributing to the model. Thus, the best models according to ADJRSQ, CP, and Forward Stepwise Regression were considered. Although all three metrics proposed different “best” models, the same first 10 variables were provided in the ADJRSQ top 3, the CP top 3, and showed significant F values in the Stepwise regression. Thus, the relevant predictor variables included: WIN_, FTR, TOV_D, DREB_, X3PT_D, AST_, EXP, PATH, ELITE_SOS, and KADJ_COMB. This proved to be the best model with exceptional fit diagnostics and a reasonable number of parameters. The final RSQUARE, ADJRSQ, MSE, and parameter estimates are shown:\n\n\n\nResults\n\n\n\n\n\nModel Plots\n\n\n\n\n\nModel Plots\n\n\nFinally, the 32 observations not included on the dataset were brought back in using the calculated beta values, the standard error, and a t-distribution value of 95% to calculate the prediction intervals for these unseen cases."
  },
  {
    "objectID": "projects/projects/March Madness Linear Models I/mm_lm1.html#conclusions",
    "href": "projects/projects/March Madness Linear Models I/mm_lm1.html#conclusions",
    "title": "March Madness Linear Modelling",
    "section": "Conclusions",
    "text": "Conclusions\nAs referenced, the NCAAM March Madness Tournament is full of surprises and no easy simulation. While our refined model successfully takes the raw data and outputs criteria indicative of a good model (ADJRSQ, RSQUARE, AIC, SBC, PRESS, CP), it would still most likely take a miracle to successfully simulate upcoming tournament outcomes. From our 32 additional observations, the model tends to under project wins. This is most likely a symptom of the fact that the majority of teams do not win a single game and that “Cinderella” picks are hard to come by. Thus, the calculation of prediction intervals helps paint a better picture of the range of wins a team may accomplish. Further analysis should be conducted to possibly standardize input and target variables to calculate which factors are the most significant as well and oversample the teams with a larger number of tournament wins."
  },
  {
    "objectID": "projects/projects/Fair Market Mapping/fair_market.html",
    "href": "projects/projects/Fair Market Mapping/fair_market.html",
    "title": "Section 8 Housing Fair Market Analyzer",
    "section": "",
    "text": "THIS PROJECT IS PRIVATIZED TO SECURE DATA REGARDING VULNERABLE HOMELESS INDIVIDUALS AND FAMILIES IN THE DETROIT AREA\nThis project involved the scripting of WebScrapers to evaluate property and utility pricing of locations that qualified for Section 8 Housing in the Detroit area. With this information, Fair Market evaluation is conducted and these locations are mapped, showing individuals and Coordinated Entry professionals locations offering fair-market rates in order to house homeless individuals.\nIf you would like to learn more, please view Community and Home Support’s Presentation at the 2024 HMIS Data Summit here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brandon Owens",
    "section": "",
    "text": "Hi! Welcome to my website. My name is Brandon Owens, and I am currently pursuing my graduate degree in Applied Statistics at Oakland University. I am honing my skills in data analysis, statistical modeling, and data-driven decision-making. My academic journey has allowed me to explore the intersection of quantitative analysis and real-world applications with particular articulation in Bayesian Inference, Natural Language Processing, and Machine Learning.\nWithin industry, I have developed a deep passion for sports, finance, and operations, driven by my desire to utilize the power of data to solve complex problems and drive meaningful decisions.\nIn my free time, you can find me playing basketball and chess, watching movies and sports, running, reading, and spending time with loved ones. If you’re interested in any of my projects or feeling like connecting, please reach out!"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Follow Along with my Picks",
    "section": "",
    "text": "Here you can follow all of my bookie picks! -updated every 24 hours!\nDisclaimer:\nThe picks provided on this page are for informational and entertainment purposes only. While I strive to offer insightful analysis, I make no guarantees regarding the outcome of any bets or picks shared here. Sports betting involves risk, and you should always wager responsibly. By following these picks, you acknowledge that you are doing so at your own risk. I am not liable for any financial losses or other consequences resulting from your betting decisions."
  },
  {
    "objectID": "education/education.html",
    "href": "education/education.html",
    "title": "Education",
    "section": "",
    "text": "GPA: 4.0\nPosition Held: Graduate Research Assistant (Data Scientist for the Career and Life Design Center)\nRelevant Coursework:\n\nPractical Computing for Data Analysis, Bayesian Data Analysis, Natural Language Processing, Applied Linear Models, Mathematical Statistics, Probability and Decision Theory, Multivariate Calculus, Linear Algebra\n\nGroups:\n\nOakland University Exercise Science Club\nOakland University Mathematical Society\n\n\n\n\n\n\n\nGPA: 4.0\nAwards/Honors:\n\nPresidential Platinum Scholar (2019-2022)\nPresident’s List (2019-2022)\n\nRelevant Coursework:\n\nIntroduction to Probability Theory, Human Physiology, Functional Anatomy, Gross Anatomy, Human Pathology, Clinical Medicine, Motor Control, Biomechanics, Microeconomics, Macroeconomics\n\nGroups:\n\nOakland University Exercise Science Club"
  },
  {
    "objectID": "education/education.html#schooling",
    "href": "education/education.html#schooling",
    "title": "Education",
    "section": "",
    "text": "GPA: 4.0\nPosition Held: Graduate Research Assistant (Data Scientist for the Career and Life Design Center)\nRelevant Coursework:\n\nPractical Computing for Data Analysis, Bayesian Data Analysis, Natural Language Processing, Applied Linear Models, Mathematical Statistics, Probability and Decision Theory, Multivariate Calculus, Linear Algebra\n\nGroups:\n\nOakland University Exercise Science Club\nOakland University Mathematical Society\n\n\n\n\n\n\n\nGPA: 4.0\nAwards/Honors:\n\nPresidential Platinum Scholar (2019-2022)\nPresident’s List (2019-2022)\n\nRelevant Coursework:\n\nIntroduction to Probability Theory, Human Physiology, Functional Anatomy, Gross Anatomy, Human Pathology, Clinical Medicine, Motor Control, Biomechanics, Microeconomics, Macroeconomics\n\nGroups:\n\nOakland University Exercise Science Club"
  },
  {
    "objectID": "education/education.html#certifications",
    "href": "education/education.html#certifications",
    "title": "Education",
    "section": "Certifications",
    "text": "Certifications\n\n\n\nMachine Learning Coursera Certification\n\n\n\nObtained: January 2025\n\n\n\n\nTableau Data Analyst Certification\n\n\n\nObtained: August 2024\n\n\n\n\nHarvard CS50 Certification\n\n\n\nObtained: September 2023\n\n\n\n\nNSCA Certified Strength and Conditioning Specialist (CSCS)\n\n\n\nObtained: December 2022"
  },
  {
    "objectID": "projects/projects/Electoral College Bayesian Inference/electoral.html",
    "href": "projects/projects/Electoral College Bayesian Inference/electoral.html",
    "title": "Electoral College Bayesian Inference 2016 Election",
    "section": "",
    "text": "With the 2024 Presidential Election approaching, polling data from the 2016 Presidential Election was utilized to create a posterior estimate of every state’s (including D.C.) popular voting pattern to try to predict the electoral college voting and determine who would win the election. One-thousand simulation draws from these posterior densities were able to correctly predict 86.5% of the region voting that was to occur during the actual election (with a final singular vote accuracy of 86.2%); which, when paired with the electoral votes for each state, was unable to predict Donald Trump’s defeat over Hillary Clinton that year."
  },
  {
    "objectID": "projects/projects/Electoral College Bayesian Inference/electoral.html#abstract",
    "href": "projects/projects/Electoral College Bayesian Inference/electoral.html#abstract",
    "title": "Electoral College Bayesian Inference 2016 Election",
    "section": "",
    "text": "With the 2024 Presidential Election approaching, polling data from the 2016 Presidential Election was utilized to create a posterior estimate of every state’s (including D.C.) popular voting pattern to try to predict the electoral college voting and determine who would win the election. One-thousand simulation draws from these posterior densities were able to correctly predict 86.5% of the region voting that was to occur during the actual election (with a final singular vote accuracy of 86.2%); which, when paired with the electoral votes for each state, was unable to predict Donald Trump’s defeat over Hillary Clinton that year."
  },
  {
    "objectID": "projects/projects/Electoral College Bayesian Inference/electoral.html#introduction",
    "href": "projects/projects/Electoral College Bayesian Inference/electoral.html#introduction",
    "title": "Electoral College Bayesian Inference 2016 Election",
    "section": "INTRODUCTION",
    "text": "INTRODUCTION\nEvery four years the United States Presidential Election sweeps the nation to discover who will be the country’s new Chief Executive. Although the U.S. is a democratic country, it is subject to a unique system known as the Electoral College. In simple terms, candidates compete in each state (and the capital region) by popular vote to obtain that state’s specific number of electoral votes that is determined by its population. Therefore, some states are worth more than other states for candidates to win and requires strategic campaigning of specific states to obtain enough electoral votes to be the victor. Because this system is unique, predicting who will win the Presidency is no easy task – simple polling of the general public does not clearly paint a picture of who is favored because the overall popular vote means nothing.\nThe election of 2016 is particularly interesting because the two lead candidates (Donald Trump and Hillary Clinton) were extremely polarizing and led to unique voting patterns. Voters seemed to either aggressively align with one of the two major parties, would vote to choose the lesser of the two polarizing figures, or settle voting on a smaller party candidate. To display just how difficult prediction was during this pattern, major polling sources like FiveThreeEight, the Upshot, 270toWin, the Princeton Election Consortium, the Associated Press, and many other sources incorrectly predicted that Hillary Clinton would be victorious."
  },
  {
    "objectID": "projects/projects/Electoral College Bayesian Inference/electoral.html#methods",
    "href": "projects/projects/Electoral College Bayesian Inference/electoral.html#methods",
    "title": "Electoral College Bayesian Inference 2016 Election",
    "section": "METHODS",
    "text": "METHODS\nThis analysis was centered from Nate Silver’s:FiveThirtyEight “2016 Election Polls” dataset on Kaggle and eventually paired with David Ward’s “US Electoral College Votes Per State 1788-2020” on Kaggle for Electoral Voting attributes. All data collected was from national polls conducted from November 2015 to November 2016 – the year leading up to the election. Only data with raw polling data with credible polling grades of “B” or above were considered by the analysis. It was assumed that participants would not change their minds from the time of the poll and that each poll maintained a solid spread of each state’s constituency. Additionally, because of the lack of diversity in the Presidency’s usual party, only the two major parties were considered (Republican Donald Trump and Democrat Hillary Clinton). Multi-region polls from the same state were condensed and all national polls that took results from a national census that was not state specific were excluded. Additionally, each candidate was assumed to have a normal prior distribution of backing from the polls as well as a normal likelihood for being voted for by voters in that state.\nWith similar idea to Brittany Alexander’s Bayesian Model for the Prediction of the U.S. Presidential Elections, the 2016 election was analyzed using each state’s conjugate normal prior distribution was taken from a specific region of the United States that the state fell into that excluded their own state. This assumes that states in similar geographical areas tend to exhibit similar voting patterns. The regions were separated according to the following image:\n\n\n\nVivid Maps Regioning\n\n\nWhere, for instance, if Texas was taken, the prior mean and variance would be taken from the samples of the rest of the Southwest Region (Arizona, New Mexico, and Oklahoma). Because mean and variance are known, our normal conjugate prior would be:\n\n\n\nPrior\n\n\nAnd the normal likelihood function would be:\n\n\n\nLikelihood\n\n\nThus, with a normal likelihood and a normal prior, our conjugate normal posterior for several observations of y, would be modeled by:\n\n\n\nPosterior\n\n\nAfter the data was cleaned and normalized between the two primary candidates, a posterior distribution for each candidate in each state was generated by 10,000 simulated values and was plotted using RStudio. From these 10,000 simulated values, 1,000 values were randomly drawn, normalized, and then used to predict the percent of the vote each candidate received from the election for each state. These simulations then were used to predict a singular outcome by counting if one candidate received over 500 decisions of the 1,000 samples. The winner of each state was then awarded that state’s electoral college votes to predict who would win the 2016 Presidential Election by having a greater sum of votes. Complete accuracy of the simulations was also recorded on a simulation by simulation basis."
  },
  {
    "objectID": "projects/projects/Electoral College Bayesian Inference/electoral.html#resultsdiscussion",
    "href": "projects/projects/Electoral College Bayesian Inference/electoral.html#resultsdiscussion",
    "title": "Electoral College Bayesian Inference 2016 Election",
    "section": "RESULTS/DISCUSSION",
    "text": "RESULTS/DISCUSSION\nFrom the 10,000 posterior draws for each state for each candidate, the total accuracy of the voting outcome on a sample by sample basis turned out to be 86.5% – which when aggregated into singular state votes lowered just slightly to 86.2% and resulted in the following electoral college voting simulation:\n\n\n\nElection Simulation Results\n\n\nThe predicted model struggled significantly in the “Great Lakes”/”Mideast” region which coincidentally lies in many swing states from this election: meaning that these states were battlegrounds that resulted in a difficult and polarized consensus on how constituents would vote. Some swing states like Colorado, Florida, Minnesota, New Hampshire, and Virginia were predicted correctly; however, others like Michigan, Ohio, Iowa, Pennsylvania, and North Carolina were not. Thus, while the model had a high accuracy rate of 86.2% it ultimately struggled to predict swing states in the Midwest United States. Future analyses would be wise to make several changes with this information. First, as mentioned, obtaining different prior distributions due to the volatility of swing states may have been useful. Additionally, accounting for all major candidates (Hillary Clinton, Donald Trump, Gary Johnson, and Evan McMullin) was ignored and while the two major parties dominated, according to FiveThreeEight, Gary Johnson was predicted to command about 5.0% of the popular vote. Normalizing candidates may have harmed this distribution as it is not known how pollees would vote if they had to choose for a candidate other than Johnson. Lastly, utilizing poll data from FiveThreeEight may have also been misinformed. While the organization is well known for their election forecasts, they also incorrectly predicted the outcome of the election.\nPosterior distributions for each state for both candidates are shown in the appendix. Hillary Clinton’s distribution is blue, Donald Trump’s distribution is red. For personal observation or reproduction, all code for analysis has been stored inside of brandonowens24’s GitHub repository labeled: “Simple Bayesian Inference on Electoral College Voting”."
  },
  {
    "objectID": "projects/projects/Handshake Timeline Reporting/handshake.html",
    "href": "projects/projects/Handshake Timeline Reporting/handshake.html",
    "title": "Handshake Timeline Reporting",
    "section": "",
    "text": "A script that uses data from Handshake reports to create a Timeline of all student engagements from 2019-present. With this timeline, the user has the ability to generate several other reports.\n\n[!NOTE]\nThis script only generates csv files with the data of each report contained. If you need these reports in a better format (Google Sheets), that will have to be done separately.\n\n\n[!IMPORTANT]\nFor the script to function, the configuration must be set properly.\n\nSee Setup for the necessary setup for these scripts to run automatically."
  },
  {
    "objectID": "projects/projects/Handshake Timeline Reporting/handshake.html#introduction",
    "href": "projects/projects/Handshake Timeline Reporting/handshake.html#introduction",
    "title": "Handshake Timeline Reporting",
    "section": "",
    "text": "A script that uses data from Handshake reports to create a Timeline of all student engagements from 2019-present. With this timeline, the user has the ability to generate several other reports.\n\n[!NOTE]\nThis script only generates csv files with the data of each report contained. If you need these reports in a better format (Google Sheets), that will have to be done separately.\n\n\n[!IMPORTANT]\nFor the script to function, the configuration must be set properly.\n\nSee Setup for the necessary setup for these scripts to run automatically."
  },
  {
    "objectID": "projects/projects/Handshake Timeline Reporting/handshake.html#features",
    "href": "projects/projects/Handshake Timeline Reporting/handshake.html#features",
    "title": "Handshake Timeline Reporting",
    "section": "Features",
    "text": "Features\nThis script can currently create a timeline and 3 different types of reports based on a user-defined config file.\n\nReport timeline of all student interactions including (Applications, Appointments, Events, Career Fairs, and Logins) from 2019-Present.\nReport student engagements following CLDC appointments.\nReport student engagements following COM1100 presentations.\nReport student engagements vs. their FDS outcomes.\n\n\nGenerated Report Types\nAll producable reports are delivered in three types of formats:\n\nTimeline: Creates a timeline of student identifiers with their engagement types and dates.\n\n\n\nStudent_ID\nDescriptive Information…\nEvent_Type\nDate\n\n\n\n\n000001\n…\nLogins\n20240101\n\n\n000001\n…\nCareer_Fairs\n20240103\n\n\n000002\n…\nAppointments\n20250102\n\n\n\n\n\nAggregate: Balances student identifier and descriptive information with timeline aggregate counts.\n\n\n\nStudent_ID\nDescriptive Information…\nTotal {Engagement Types}\n\n\n\n\n000001\n…\n1\n\n\n000002\n…\n2\n\n\n000003\n…\n1\n\n\n\n\n\n\nMelted: Takes the aggregated report and melts it for use in different visualization softwares.\n\n\n\n\n\n\n\n\n\n\nStudent_ID\nDescriptive Information…\nEvent_Type\nCount\nYear\n\n\n\n\n000001\n…\nLogins\n10\nFreshman\n\n\n000001\n…\nLogins\n12\nSophomore\n\n\n000001\n…\nLogins\n15\nJunior\n\n\n\n\n\n\n\nReport Options\n\nTimeline\nA timeline following the timeline format in Report Types of all student engagements in the following categories taken from scheduled Handshake Reports: * Applications * Appointments * Career_Fairs * Events * Logins\n\n\nCLDC\nProduces two reports that summarize student engagement following a CLDC appointment. The two reports are in the following Report Types formats: * Aggregate * Melt\n\n\nCOM1100\nProduces four reports that summarize student engagement following 1+ COM1100 presentations and no COM1100 presentations. Each of the conditions have 2 reports following Report Types formats: * Aggregate * Melt\n\n\nFDS\nProduces two reports that summarize past student engagement for students that have filled out their First Destination Surverys. The two reports are in the following Report Types formats: * Aggregate * Melt"
  },
  {
    "objectID": "projects/projects/Handshake Timeline Reporting/handshake.html#what-happens-to-the-data",
    "href": "projects/projects/Handshake Timeline Reporting/handshake.html#what-happens-to-the-data",
    "title": "Handshake Timeline Reporting",
    "section": "What Happens to the Data?",
    "text": "What Happens to the Data?\nAfter these reports are created, they are streamlined into PowerBI Dashboards that are effectively used by Oakland University’s Career and Life Design Center."
  },
  {
    "objectID": "projects/projects/Movie Recommendation App/movie.html",
    "href": "projects/projects/Movie Recommendation App/movie.html",
    "title": "Movie Recommendation App",
    "section": "",
    "text": "See Huggingface Spaces to run the app!\n\n\nSimply enter a valid movie or show (with the correct type option selected), the model you wish to use to find similar movies or shows, and the method of similarity.\nThen, click submit and in 1-5 seconds, the model will return 5 movies or shows (with their respective posters) that are the most similar to your inputted movie or show!\n\n\n\nOutput\n\n\n\n\n\n\nMake sure that if you are entering a movie, the Movie tab is selected (and the Show tab for shows).\n\nIt is possible that your movie/show doesn’t exist in the database (last updated 4/22/2024).\n\n\n\nEx: “No Country for Old Men” is a movie and must have the Movie button selected under type.\n\n\nMake sure the title is formatted correctly:\n\nCaps do not matter\nPunctuation and numbers do\n\n\n\nEx: Correct: Star Wars: Episode III – Revenge of the Sith\nEx: Correct: star wars: episode III - revenge of the sith\nEx: Incorrect: Star Wars Episode 3 - Revenge of the sith\n\n\n\n\n\n\nFlowchart\n\n\nTo create the application, data was scraped from TMDB API to obtain the names of movies and television shows. With the desired names and ids, plot information was scraped from the OMBD API. This resulted in ~1.02 million movies and ~170k shows in my dataset.\nI then processed the information to get a training corpus. This involved dropping missing columns such as “Title”, “Plot”, “Genre”, “Type”, and “Director”, dropping “Title” duplicates, filtering for origin country to be “United States” or “Japan” or the filtered original language being “English”. My features were then created from a column I named “Description” that fused together the genre, type, director, plot, and keywords for each movie/show. The format looked something like: “A {x[‘Genre’]} {x[‘Type’]} directed by {x[‘Director’]} with keywords consisting of {x[‘keywords’]}. {x[‘Plot’]}”`. This left me with ~200k movies and ~20k shows. I then tokenized these descriptions, lowercasing and removing punctuation.\nI tried a purely NLP approach – no other features were encoded besides these descriptions. Because of the timeline for getting this project in, I didn’t have the time to incorporate other features or test compared to pre-trained word embeddings. This may have been slightly problematic because it creates a bias and dependency on how well the movie’s “full” plot description captures the information of the movie. Perhaps some movies or shows give less information to create mystery and interest and perhaps some writers are just bad at summarization tasks.\nI then took these corpora and applied them separately to 3 different models with different similarity metrics:\n\nModels\n\nWord2Vec Count: Trained movie/show corpus on Gensim’s Word2Vec. In order to get the document embedding, the word vectors for the description are averaged.\nWord2Vec TFIDF: Trained movie/show corpus on Gensim’s Word2Vec. In order to get the document embedding, the word vectors are multiplied by a TFIDF vectorizer and then averaged.\nDoc2Vec: Trained movie/show corpus on Gensim’s Doc2Vec.\n\nSimilarity Metric\n\nCosine: Utilizes cosine similarity to retrieve the top 5 most similar movies/shows.\nEuclidian: Utilizes the distance formula to find the smallest distance between movie/show embeddings to retrieve the top 5 most similar movies/shows.\n\n\nKeep in mind, the only features used for these models was the description column that I made for each movie and show (and modeled separately – movies and shows apart). I saved the Doc2Vec models and the different document embeddings for the separate word2vec models as numpy matrices for faster processing in my Gradio app. These were all imported to Huggingface spaces where I then built my Gradio app seen on Huggingface.\n\n\n\nWhich model works the best? It depends.\nBecause this is a recommendation system and I don’t have access to any sort of user data, I think the easiest, quick, and dirty way to see which metrics perform the best is to run the tool with my own preferences. I wanted to query my app with 5 movies I like and 5 shows I also enjoy (about different genres). For each of these movies and shows using each model and similarity metric, I wanted to see how many of the 5 recommendations the system outputted that I have watched (indicating it was successful), sounded similar and relevant to my input’s plot, or sounded interesting and related. I then added this number of across all of the movies and all of the shows to get an accuracy for each model regarding each type separately.\nI ended up choosing the mediums: * No Country for Old Men (Western, Thriller) * Interstellar (Sci-Fi) * When Harry Met Sally… (Romance) * Whiplash (Drama, Music) * The Other Guys (Comedy) * Fallout (Post-Apocalyptic, Adventure) * Avatar the Last Airbender (Animated) * Better Call Saul (Drama, Crime) * Breaking Bad (Drama, Crime) * Attack on Titan (Animated, War, Action)\nAlthough this isn’t the largest cluster of data to sample from, the results looked like this…\n\n\n\nEvaluation\n\n\nModeling that the Word2Vec Count model with cosine similarity had the best accuracy for both the movies and shows provided. While this may be subject to change with more data added, especially because the Euclidian metric and the Word2Vec TFIDF models were pretty close, it becomes pretty apparent that the Doc2Vec model underperforms in both areas. Thus, it would be advised to use one of the Word2Vec models when trying to find similar entertainment mediums (which is ironic because it took by far the most time to train). Additionally, the accuracy from movies to shows is a bit alarming. It seems as though the shows really struggled to show some interesting similar shows compared to the movies. This is most likely because there were only ~20,000 show descriptions to use for modeling as opposed to roughly 10x that many for movies.\n\n\n\n\nThe TMDB Movies and Shows datasets from Kaggle allowed for me to not have to scrape the entiretly of TMDB.\nThe idea of using a TFIDF approach instead of just comparing to pre-trained embeddings was found from Dhilip Subramanian’s Book Recommendation System.\nAll models were built using Gensim.\n\n\n\n\n\nShow Models:\n\nWith any future production, it would be useful to instead combine the shows and movies when training and add one-hot encoding for other features, like if it is a series or a movie.\n\nLack of Comparison:\n\nShould be compared to pre-trained models like GloVe or Google open word embeddings would be useful to see if training a Word2Vec or Doc2Vec model was more accurate than one that was trained by a big company with a much larger amount of data and more time.\n\nFocus on improving compute time\nDidn’t add features to add additional filtering (language, country, minimum rating, etc.)\n\n\n\n\n\nAll models were trained on an NVIDIA RTX 2070 GPU.\nCheck out the configuration reference at https://huggingface.co/docs/hub/spaces-config-reference"
  },
  {
    "objectID": "projects/projects/Movie Recommendation App/movie.html#using-the-app",
    "href": "projects/projects/Movie Recommendation App/movie.html#using-the-app",
    "title": "Movie Recommendation App",
    "section": "",
    "text": "Simply enter a valid movie or show (with the correct type option selected), the model you wish to use to find similar movies or shows, and the method of similarity.\nThen, click submit and in 1-5 seconds, the model will return 5 movies or shows (with their respective posters) that are the most similar to your inputted movie or show!\n\n\n\nOutput"
  },
  {
    "objectID": "projects/projects/Movie Recommendation App/movie.html#having-trouble",
    "href": "projects/projects/Movie Recommendation App/movie.html#having-trouble",
    "title": "Movie Recommendation App",
    "section": "",
    "text": "Make sure that if you are entering a movie, the Movie tab is selected (and the Show tab for shows).\n\nIt is possible that your movie/show doesn’t exist in the database (last updated 4/22/2024).\n\n\n\nEx: “No Country for Old Men” is a movie and must have the Movie button selected under type.\n\n\nMake sure the title is formatted correctly:\n\nCaps do not matter\nPunctuation and numbers do\n\n\n\nEx: Correct: Star Wars: Episode III – Revenge of the Sith\nEx: Correct: star wars: episode III - revenge of the sith\nEx: Incorrect: Star Wars Episode 3 - Revenge of the sith\n\n\n\n\n\n\nFlowchart\n\n\nTo create the application, data was scraped from TMDB API to obtain the names of movies and television shows. With the desired names and ids, plot information was scraped from the OMBD API. This resulted in ~1.02 million movies and ~170k shows in my dataset.\nI then processed the information to get a training corpus. This involved dropping missing columns such as “Title”, “Plot”, “Genre”, “Type”, and “Director”, dropping “Title” duplicates, filtering for origin country to be “United States” or “Japan” or the filtered original language being “English”. My features were then created from a column I named “Description” that fused together the genre, type, director, plot, and keywords for each movie/show. The format looked something like: “A {x[‘Genre’]} {x[‘Type’]} directed by {x[‘Director’]} with keywords consisting of {x[‘keywords’]}. {x[‘Plot’]}”`. This left me with ~200k movies and ~20k shows. I then tokenized these descriptions, lowercasing and removing punctuation.\nI tried a purely NLP approach – no other features were encoded besides these descriptions. Because of the timeline for getting this project in, I didn’t have the time to incorporate other features or test compared to pre-trained word embeddings. This may have been slightly problematic because it creates a bias and dependency on how well the movie’s “full” plot description captures the information of the movie. Perhaps some movies or shows give less information to create mystery and interest and perhaps some writers are just bad at summarization tasks.\nI then took these corpora and applied them separately to 3 different models with different similarity metrics:\n\nModels\n\nWord2Vec Count: Trained movie/show corpus on Gensim’s Word2Vec. In order to get the document embedding, the word vectors for the description are averaged.\nWord2Vec TFIDF: Trained movie/show corpus on Gensim’s Word2Vec. In order to get the document embedding, the word vectors are multiplied by a TFIDF vectorizer and then averaged.\nDoc2Vec: Trained movie/show corpus on Gensim’s Doc2Vec.\n\nSimilarity Metric\n\nCosine: Utilizes cosine similarity to retrieve the top 5 most similar movies/shows.\nEuclidian: Utilizes the distance formula to find the smallest distance between movie/show embeddings to retrieve the top 5 most similar movies/shows.\n\n\nKeep in mind, the only features used for these models was the description column that I made for each movie and show (and modeled separately – movies and shows apart). I saved the Doc2Vec models and the different document embeddings for the separate word2vec models as numpy matrices for faster processing in my Gradio app. These were all imported to Huggingface spaces where I then built my Gradio app seen on Huggingface.\n\n\n\nWhich model works the best? It depends.\nBecause this is a recommendation system and I don’t have access to any sort of user data, I think the easiest, quick, and dirty way to see which metrics perform the best is to run the tool with my own preferences. I wanted to query my app with 5 movies I like and 5 shows I also enjoy (about different genres). For each of these movies and shows using each model and similarity metric, I wanted to see how many of the 5 recommendations the system outputted that I have watched (indicating it was successful), sounded similar and relevant to my input’s plot, or sounded interesting and related. I then added this number of across all of the movies and all of the shows to get an accuracy for each model regarding each type separately.\nI ended up choosing the mediums: * No Country for Old Men (Western, Thriller) * Interstellar (Sci-Fi) * When Harry Met Sally… (Romance) * Whiplash (Drama, Music) * The Other Guys (Comedy) * Fallout (Post-Apocalyptic, Adventure) * Avatar the Last Airbender (Animated) * Better Call Saul (Drama, Crime) * Breaking Bad (Drama, Crime) * Attack on Titan (Animated, War, Action)\nAlthough this isn’t the largest cluster of data to sample from, the results looked like this…\n\n\n\nEvaluation\n\n\nModeling that the Word2Vec Count model with cosine similarity had the best accuracy for both the movies and shows provided. While this may be subject to change with more data added, especially because the Euclidian metric and the Word2Vec TFIDF models were pretty close, it becomes pretty apparent that the Doc2Vec model underperforms in both areas. Thus, it would be advised to use one of the Word2Vec models when trying to find similar entertainment mediums (which is ironic because it took by far the most time to train). Additionally, the accuracy from movies to shows is a bit alarming. It seems as though the shows really struggled to show some interesting similar shows compared to the movies. This is most likely because there were only ~20,000 show descriptions to use for modeling as opposed to roughly 10x that many for movies.\n\n\n\n\nThe TMDB Movies and Shows datasets from Kaggle allowed for me to not have to scrape the entiretly of TMDB.\nThe idea of using a TFIDF approach instead of just comparing to pre-trained embeddings was found from Dhilip Subramanian’s Book Recommendation System.\nAll models were built using Gensim.\n\n\n\n\n\nShow Models:\n\nWith any future production, it would be useful to instead combine the shows and movies when training and add one-hot encoding for other features, like if it is a series or a movie.\n\nLack of Comparison:\n\nShould be compared to pre-trained models like GloVe or Google open word embeddings would be useful to see if training a Word2Vec or Doc2Vec model was more accurate than one that was trained by a big company with a much larger amount of data and more time.\n\nFocus on improving compute time\nDidn’t add features to add additional filtering (language, country, minimum rating, etc.)\n\n\n\n\n\nAll models were trained on an NVIDIA RTX 2070 GPU.\nCheck out the configuration reference at https://huggingface.co/docs/hub/spaces-config-reference"
  },
  {
    "objectID": "projects/projects/NFL Injuries Bayesian Analysis/nfl_injuries_bayesian.html",
    "href": "projects/projects/NFL Injuries Bayesian Analysis/nfl_injuries_bayesian.html",
    "title": "NFL Injuries Bayesian Analysis",
    "section": "",
    "text": "The NFL is valued as a $163B organization where athletes compete for a total of 20-24 games during the season (3 preseason, 17 in-season, 0-4 post-season games). A major complaint of fans over the last couple of years has surrounded the significance of injuries on the sport in several ways:\n\nAthlete Well-Being:\n\nThe physical and mental health of human beings is important.\nInjuries impact an athlete’s ability to live during and after their careers.\nNew research is demonstrating the tragic significance of once unforseen traumas, like concussions.\n\nOrganizational Cost:\n\nWhen star players that fandoms are build around are injured, this devaules the product that the NFL sells.\nOrganizations want players to be hurt to maintain fan interest and to not shell out “guaranteed” money for player contracts when those players aren’t participating.\n\n\nWith that being said, the NFL has instated over 50 rule changes to reduce player danger since 2002 alone."
  },
  {
    "objectID": "projects/projects/NFL Injuries Bayesian Analysis/nfl_injuries_bayesian.html#background",
    "href": "projects/projects/NFL Injuries Bayesian Analysis/nfl_injuries_bayesian.html#background",
    "title": "NFL Injuries Bayesian Analysis",
    "section": "",
    "text": "The NFL is valued as a $163B organization where athletes compete for a total of 20-24 games during the season (3 preseason, 17 in-season, 0-4 post-season games). A major complaint of fans over the last couple of years has surrounded the significance of injuries on the sport in several ways:\n\nAthlete Well-Being:\n\nThe physical and mental health of human beings is important.\nInjuries impact an athlete’s ability to live during and after their careers.\nNew research is demonstrating the tragic significance of once unforseen traumas, like concussions.\n\nOrganizational Cost:\n\nWhen star players that fandoms are build around are injured, this devaules the product that the NFL sells.\nOrganizations want players to be hurt to maintain fan interest and to not shell out “guaranteed” money for player contracts when those players aren’t participating.\n\n\nWith that being said, the NFL has instated over 50 rule changes to reduce player danger since 2002 alone."
  },
  {
    "objectID": "projects/projects/NFL Injuries Bayesian Analysis/nfl_injuries_bayesian.html#what-are-we-looking-at",
    "href": "projects/projects/NFL Injuries Bayesian Analysis/nfl_injuries_bayesian.html#what-are-we-looking-at",
    "title": "NFL Injuries Bayesian Analysis",
    "section": "What Are We Looking At?",
    "text": "What Are We Looking At?\nMany of these rule changes have caused significant controversy over the last handful of seasons (just check Twitter). With this divide, many fans believe that the league is taking the fun out of the game by limiting exciting plays through the elimination of violence and contact. Using prior information from the 2009-2013 seasons, we inform multiple models (beta congjugate, dirichlet conjugate, and another beta conjugate model) to gain insight on whether protocols set by the NFL to reduce player injury are successful or not over the last 10 years."
  },
  {
    "objectID": "projects/projects/NFL Injuries Bayesian Analysis/nfl_injuries_bayesian.html#findings",
    "href": "projects/projects/NFL Injuries Bayesian Analysis/nfl_injuries_bayesian.html#findings",
    "title": "NFL Injuries Bayesian Analysis",
    "section": "Findings",
    "text": "Findings\n\n\n\nPosterior Distribution\n\n\nUtilizing a Beta Conjugate Posterior model, the most massive of the posterior distribution shifts comes in 2016, where previously the distributions greatly reflected the prior. With this large shift, it is hard to tell if one rule specifically caused the change (clipping/kickoff format), both rules together did, or with the new rules, there was a crackdown on officiating. This change could also have been caused by unforseen factors, like age.\n\n\n\nPosterior Distribution 2\n\n\nAmid speculation that age is an unforseen factor (perhaps there is a greater proportion of older, more injury prone players present in the league), we utilize a Dirichlet Conjugate Posterior Distribution for four different age bins to gain insight on what distributions of ages occupy the NFL. It is discovered that the percent makeup of 30+ year olds in the league remains around a constant theta distribution; however, the number of 20-24 year olds and 25-29 year olds seams to shift and then realign with the start of the decade.\n\n\n\nPosterior Distribution 3\n\n\nBecause of the slight change in 20-24 year old distribution, we decide to check on the percentage of players that get hurt compared to all players present that are that age. The results are pretty alarming and display that athletes from 20-24 years old have a much more likely chance of becoming injured. Whether this is due load management, increased volume compared to older players, the verocity of a new, intense league, newer sports science training that has progressed explosive movement too fast, or other reasons, it is unclear.\nWhat is clear from these findings is that if the NFL would like to continue their reduction in player injuries, it may be beneficial to commit resources, analysis, and procedure into preventing younger players from getting injured as they have been able to limit the percent chance of older players getting hurt. In further, from our three posterior distribution functions and many other exploratory data wranglings and visualizations, we are able to demonstrate that there has been a definite shift in the injury rate in the NFL in the last 10 years for the better. While it seems like injuries to younger players have gotten worse and the influx of younger athletes has swung, the injury likelihood for older athletes has significantly dropped. This demonstrates that age is a factor for players getting injured, but doesn’t quite explain the dropoff of injury rate within the league. Other factors should be explored to fully assert the claim that NFL rules are indeed reducing injury likelihood amongst athletes. Meanwhile, the findings from this analysis have demonstrates a need for better rehabiliation, therapy, and training methods for younger athletes entering the NFL."
  },
  {
    "objectID": "projects/projects/Supreme Court/supreme_court.html",
    "href": "projects/projects/Supreme Court/supreme_court.html",
    "title": "Supreme Court Decision Predictor",
    "section": "",
    "text": "For our MIS 5470: Practical Computing for Data Analytics Course.\nWe looked to see if we could successfully predict the outcome of Supreme Court Case Rulings using factors and Natural Language processing of the facts statements given by the court.\nWe performed data wrangling, manipulation, feature engineering, exploratory data analysis, data visualization, and predictive modelling to gain an understanding to generate predictions using a Supreme Court Cases Dataset in order to predict the outcome of major Supreme Court decisions (first party winner: the first party being the appellant)."
  },
  {
    "objectID": "projects/projects/Supreme Court/supreme_court.html#description",
    "href": "projects/projects/Supreme Court/supreme_court.html#description",
    "title": "Supreme Court Decision Predictor",
    "section": "",
    "text": "For our MIS 5470: Practical Computing for Data Analytics Course.\nWe looked to see if we could successfully predict the outcome of Supreme Court Case Rulings using factors and Natural Language processing of the facts statements given by the court.\nWe performed data wrangling, manipulation, feature engineering, exploratory data analysis, data visualization, and predictive modelling to gain an understanding to generate predictions using a Supreme Court Cases Dataset in order to predict the outcome of major Supreme Court decisions (first party winner: the first party being the appellant)."
  },
  {
    "objectID": "projects/projects/Supreme Court/supreme_court.html#results",
    "href": "projects/projects/Supreme Court/supreme_court.html#results",
    "title": "Supreme Court Decision Predictor",
    "section": "Results",
    "text": "Results\n\n\n\nResults\n\n\nThe baseline model of assuming no lower court decision reversals is able to predict the first party winner with 64.0% accuracy.\nThe NLP only Counts-BOW Random Forest outperformed all other tests (marginally) with an accuracy of 66.8%, being closely followed by the 66.7% logisitic regression missing NLP features and the 66.7% accuracy with TFIDF BOW logistic regression. Several models were able to slightly outperform (up to 2.8%).\nThus, it can be shown that modeling using Facts Statements of Supreme Court Cases provides insights towards voting of SCOTUS cases with improved metrics as opposed by the simple assumption of party-dominated voting."
  }
]